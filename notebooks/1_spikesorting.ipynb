{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NWB-Datajoint tutorial 1\n",
    "\n",
    "**Note: make a copy of this notebook and run the copy to avoid git conflicts in the future**\n",
    "\n",
    "This is the first in a multi-part tutorial on the NWB-Datajoint pipeline used in Loren Frank's lab, UCSF. It demonstrates how to run spike sorting within the pipeline.\n",
    "\n",
    "If you have not done [tutorial 0](0_intro.ipynb) yet, make sure to do so before proceeding.\n",
    "\n",
    "Let's start by importing the `nwb_datajoint` package, along with a few others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import nwb_datajoint as nd\n",
    "\n",
    "# ignore datajoint+jupyter async warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=ResourceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tables so that we can call them easily\n",
    "from nwb_datajoint.common import (RawPosition, HeadDir, Speed, LinPos, StateScriptFile, VideoFile,\n",
    "                                  DataAcquisitionDevice, CameraDevice, Probe,\n",
    "                                  DIOEvents,\n",
    "                                  ElectrodeGroup, Electrode, Raw, SampleCount,\n",
    "                                  LFPSelection, LFP, LFPBandSelection, LFPBand,\n",
    "                                  SortGroup, SpikeSorting, SpikeSorter, SpikeSorterParameters,\n",
    "                                  SpikeSortingWaveformParameters, SpikeSortingArtifactParameters,\n",
    "                                  SpikeSortingParameters,SpikeSortingMetrics, CuratedSpikeSorting,\n",
    "                                  FirFilter,\n",
    "                                  IntervalList, SortInterval,\n",
    "                                  Lab, LabMember, Institution,\n",
    "                                  BrainRegion,\n",
    "                                  SensorData,\n",
    "                                  Session, ExperimenterList,\n",
    "                                  Subject,\n",
    "                                  Task, TaskEpoch,\n",
    "                                  Nwbfile, AnalysisNwbfile, NwbfileKachery, AnalysisNwbfileKachery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will continue to work with the copy of `beans20190718.nwb` that you created in tutorial 0. If you deleted it from `Session`, make sure to re-insert before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the file that you copied and renamed; make sure it's something unique. \n",
    "nwb_file_name = 'beans20190718.nwb'\n",
    "filename, file_extension = os.path.splitext(nwb_file_name)\n",
    "# This is a copy of the original nwb file, except it doesn't contain the raw data (for storage reasons)\n",
    "nwb_file_name2 = filename + '_' + file_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run if you need to reinsert the data - otherwise skip\n",
    "nd.insert_sessions(nwb_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike sorting\n",
    "\n",
    "In general, running spike sorting means making decisions about the following:\n",
    "1. which eletrodes to sort together (e.g. electrodes that form a tetrode should be sorted together, but tetrodes that are far apart need not be);\n",
    "2. which time interval to sort (e.g. there may a long period in the recording where nothing happens, and we might want to exclude that);\n",
    "3. which spike sorter to use (e.g. Mountainsort? Kilosort? IronClust?);\n",
    "4. given choice of the spike sorter in 3, which parameter set to use.\n",
    "\n",
    "In our Datajoint framework, everything that we do is an interaction with a table. This is true for spike sorting as well - i.e. we think of spike sorting as a process where we enter parameters of spike sorting (i.e. our decisions about the four questions above) into tables, and use that information to populate another table that will hold the result of spike sorting. Under the hood, we use a number of packages, notably `spikeinterface`. But the user need not know this - they just have to interact with the table. This makes spike sorting straightforward. In addition, the entries in these tables serve as a record of exactly which decisions you made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define sort group\n",
    "We start with the first question: which electrodes do we want to sort together? We first inspect the `Electrode` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Electrode & {'nwb_file_name': nwb_file_name2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recording was done with polymer probes. You can see that from `probe_type` attribute, which is `128c-4s8mm6cm-20um-40um-sl`: 128-channels, 4-shank probe, with 8-mm shank length, 6-cm probe length, 20 $\\mu$m electrode diameter, and 40 $\\mu$m inter-electrode distance, arranged in a single layer. In this case, an `electrode_group_name` refers to a probe (i.e. all the rows with `electrode_group_name` 0 are electrodes that are in the first probe). We can see that two probes (`0` and `1`) were used in this recording:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique probe id\n",
    "np.unique((Electrode & {'nwb_file_name': nwb_file_name2}).fetch('electrode_group_name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and each probe indeed has four shanks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique shank id for the first probe\n",
    "np.unique((Electrode & {'nwb_file_name': nwb_file_name2, 'electrode_group_name': 0}).fetch('probe_shank'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our job is to identify the electrodes that we want to sort together, and add them as a sort group in the `SortGroup` table. One natural way to do this is to set each shank as a sort group (for tetrode recordings, each tetrode can be thought of as a \"shank\" with four electrodes). Use `set_group_by_shank` method for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SortGroup().set_group_by_shank(nwb_file_name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates 8 sort groups, one for each of the four shanks in the two probes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SortGroup & {'nwb_file_name': nwb_file_name2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SortGroup` has a *parts table* called `SortGroupElectrode` - think of this as child table that contains information auxiliary to the parent table. As you can see, it contains two extra attributes: `electrode_group_name` and `electrode_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SortGroup.SortGroupElectrode & {'nwb_file_name': nwb_file_name2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you don't want to sort by shank? Maybe you want to select specific electrodes across shanks and sort them. To do so, you just have to manually `insert` a new entry into the `SortGroup` and `SortGroupElectrode` tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since this is the ninth sort group, we will use 8 as its id (remember the zero based indexing)\n",
    "sort_group_id = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we make a new entry in the SortGroup table, and give it the sort_group_id.\n",
    "SortGroup.insert1({'nwb_file_name': nwb_file_name2, 'sort_group_id': sort_group_id, 'sort_reference_electrode_id': -1}, \n",
    "                  skip_duplicates = True)\n",
    "# Next, we will associate with the sort group that we just created \n",
    "# every fourth electrode of the first shank.\n",
    "# The entries will created as a nested list and added:\n",
    "SortGroup.SortGroupElectrode.insert([[nwb_file_name2, 8, 0, elec] for elec in range(0,32,4)],\n",
    "                                    skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `insert` is a method, just like `fetch`. You can insert an entry in the form of a dictionary or a list in the order of the attributes. We can look at the new entries we just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SortGroup & {'nwb_file_name': nwb_file_name2, 'sort_group_id': sort_group_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SortGroup.SortGroupElectrode & {'nwb_file_name': nwb_file_name2, 'sort_group_id': sort_group_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define sort interval\n",
    "Next, we make a decision about the time interval for our spike sorting. Let's re-examine `IntervalList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntervalList & {'nwb_file_name' : nwb_file_name2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, let's choose the first 10 seconds of the first run interval (`02_r1`) as our sort interval. To do so, we first fetch `valid_times` of this interval, define our new sort interval, and add this to the `SortInterval` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_list_name = '02_r1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = (IntervalList & {'nwb_file_name' : nwb_file_name2,\n",
    "                            'interval_list_name' : interval_list_name}).fetch1('valid_times')\n",
    "print(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_interval = np.asarray([interval[0][0]+10, interval[0][0]+20])\n",
    "print(sort_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out SortInterval\n",
    "SortInterval & {'nwb_file_name' : nwb_file_name2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_interval_name = 'beans_02_r1_10s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the required attributes.\n",
    "# This time, the entries take the form of a dictionary.\n",
    "SortInterval.insert1({'nwb_file_name' : nwb_file_name2,\n",
    "                      'sort_interval_name' : sort_interval_name,\n",
    "                      'sort_interval' : sort_interval}, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See results\n",
    "SortInterval & {'nwb_file_name' : nwb_file_name2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define sorter\n",
    "Next we decide which spike sorter to use. This boils down to looking at the `SpikeSorter` table and choosing the one we like. Initially, `SpikeSorter` may not be populated; in that case, we insert some sorters to it by checking which ones are available via `spikeinterface`, the package that we will be using implicitly for spike sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSorter().insert_from_spikeinterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSorter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, we will be using `mountainsort4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorter_name='mountainsort4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define sorter parameters\n",
    "Once we have decided on a spike sorter, we have to set parameters. Some of these parameters are common to all sorters (e.g. frequency band to filter the raw data before sorting begins) but most are specific to the sorter that we chose. Again, we populate `SpikeSorterParameters` table with some default parameters for each sorter, and then we add our version as a new entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSorterParameters().insert_from_spikeinterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSorterParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new set of spike sorter parameters from default and add to table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the default params\n",
    "ms4_default_params = (SpikeSorterParameters & {'sorter_name' : sorter_name,\n",
    "                                               'parameter_set_name' : 'default'}).fetch1()\n",
    "print(ms4_default_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the default params\n",
    "param_dict = ms4_default_params['parameter_dict']\n",
    "# Detect both upward and downward going spikes\n",
    "param_dict['detect_sign'] = 0\n",
    "# We will sort electrodes together that are within 100 microns of each other\n",
    "param_dict['adjacency_radius'] = 100\n",
    "param_dict['curation'] = False\n",
    "# Turn filter off since we will filter it prior to starting sort\n",
    "param_dict['filter'] = False\n",
    "# Turn whiten off since we will whiten it prior to starting sort\n",
    "param_dict['whiten'] = False\n",
    "# set num_workers to be the same number as the number of electrodes\n",
    "param_dict['num_workers'] = len((SortGroup.SortGroupElectrode & {'sort_group_id':sort_group_id}).fetch('electrode_id'))\n",
    "param_dict['verbose'] = True\n",
    "# set clip size as number of samples for 2 milliseconds\n",
    "param_dict['clip_size'] = np.int(2e-3 * (Raw & {'nwb_file_name' : nwb_file_name2}).fetch1('sampling_rate'))\n",
    "param_dict['noise_overlap_threshold'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same filter_parameter_dict\n",
    "filter_param_dict = ms4_default_params['parameter_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a unique name here\n",
    "parameter_set_name = 'beans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert\n",
    "SpikeSorterParameters.insert1({'sorter_name': sorter_name,\n",
    "                               'parameter_set_name': parameter_set_name,\n",
    "                               'parameter_dict': param_dict,\n",
    "                               'filter_parameter_dict': filter_param_dict,}, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that insert was successful\n",
    "SpikeSorterParameters & {'sorter_name': sorter_name, 'parameter_set_name': parameter_set_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define quality metric parameters\n",
    "\n",
    "We're almost done. There are more parameters related to how to compute the quality metrics for curation. We just use the default options here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSortingMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_metrics_list_name = 'franklab_default_cluster_metrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define artifact masking parameters\n",
    "Sometimes there are large transients in the recording due to the animal's movement or interaction with the environment. We set some parameters to mask these. Again we use the default in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSortingArtifactParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_param_name = 'default'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing everything together\n",
    "\n",
    "We now collect all the decisions we made up to here and put it into `SpikeSortingParameters` table (note: this is different from spike sor*ter* parameters defined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the params\n",
    "key = dict()\n",
    "key['nwb_file_name'] = nwb_file_name2\n",
    "key['sort_group_id'] = sort_group_id\n",
    "key['sorter_name'] = sorter_name\n",
    "key['parameter_set_name'] = parameter_set_name\n",
    "key['sort_interval_name'] = sort_interval_name\n",
    "key['artifact_param_name'] = artifact_param_name\n",
    "key['cluster_metrics_list_name'] = cluster_metrics_list_name\n",
    "key['interval_list_name'] = interval_list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSortingParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert\n",
    "SpikeSortingParameters.insert1(key, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect\n",
    "SpikeSortingParameters & {'nwb_file_name' : nwb_file_name2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running spike sorting\n",
    "Now we can run spike sorting. As we said it's nothing more than populating another table (`SpikeSorting`) from the entries of `SpikeSortingParameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify entry (otherwise runs everything in SpikeSortingParameters)\n",
    "# `proj` gives you primary key\n",
    "SpikeSorting.populate([(SpikeSortingParameters & {'nwb_file_name' : nwb_file_name2}).proj()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it worked\n",
    "SpikeSorting()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
